{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box, visualization\n",
    "from random import SystemRandom\n",
    "\n",
    "import wandb\n",
    "from wandb import init\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "\n",
    "split_at: float = 0.35\n",
    "batch_size: int = 24\n",
    "\n",
    "learning_rate: float = 0.002\n",
    "\n",
    "epoch: int = 1\n",
    "global_clip_at: float = 10.0\n",
    "\n",
    "boundary_format: str = 'xyxy'\n",
    "\n",
    "labels: list = [\n",
    "    'Dog',\n",
    "    'Dog tag',\n",
    "    'Collar'\n",
    "]\n",
    "\n",
    "labels_mapped: dict = dict(\n",
    "    zip(\n",
    "        range(len(labels)), \n",
    "        labels\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_paths: dict = dict(\n",
    "    {\n",
    "        'images': 'D:\\\\DataSet\\\\Tracking Madsen\\\\Images',\n",
    "        'annotations': 'D:\\\\DataSet\\\\Tracking Madsen\\\\Annotations'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "target_size_width: int = 640 \n",
    "target_size_height: int = 640\n",
    "\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "\n",
    "init(\n",
    "    project='YOLO', \n",
    "    entity='designermadsen',\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        join(dataset_paths['annotations'], file_name)\n",
    "        for file_name in listdir(dataset_paths['annotations'])\n",
    "        if file_name.lower().endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "size_of_annotations = len(xml_files)\n",
    "\n",
    "image_files = sorted(\n",
    "    [\n",
    "        join(dataset_paths['images'], file_name)\n",
    "        for file_name in listdir(dataset_paths['images'])\n",
    "        if file_name.lower().endswith(\".jpg\") or file_name.lower().endswith(\".jpeg\") or file_name.lower().endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def parse_xml(\n",
    "    xml_file\n",
    "):\n",
    "    global dataset_paths, labels_mapped\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = join(dataset_paths['images'], image_name)\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "\n",
    "    for object in root.iter('object'):\n",
    "        cls = object.find(\"name\").text\n",
    "        classes.append(cls)\n",
    "\n",
    "        bounderies = object.find(\"bndbox\")\n",
    "        xmin: float = float(bounderies.find('xmin').text)\n",
    "        ymin: float = float(bounderies.find('ymin').text)\n",
    "\n",
    "        xmax: float = float(bounderies.find('xmax').text)\n",
    "        ymax: float = float(bounderies.find('ymax').text)\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    label_ids: list = [\n",
    "        list(labels_mapped.keys())[list(labels_mapped.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    \n",
    "    return image_path, boxes, label_ids\n",
    "\n",
    "image_paths = []\n",
    "boundaries = []\n",
    "classes = []\n",
    "\n",
    "for file in tqdm(\n",
    "    xml_files\n",
    "):\n",
    "    image_path, boxes, label_ids = parse_xml(\n",
    "        file\n",
    "    )\n",
    "    \n",
    "    image_paths.append(\n",
    "        image_path\n",
    "    )\n",
    "    \n",
    "    boundaries.append(\n",
    "        boxes\n",
    "    )\n",
    "    \n",
    "    classes.append(\n",
    "        label_ids\n",
    "    )\n",
    "\n",
    "from tensorflow.ragged import constant\n",
    "\n",
    "image_paths = constant(image_paths)\n",
    "classes = constant(classes)\n",
    "boundaries = constant(boundaries)\n",
    "\n",
    "dataset = tensorflow.data.Dataset.from_tensor_slices((\n",
    "    image_paths,\n",
    "    classes,\n",
    "    boundaries\n",
    "))\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tensorflow.io.read_file(image_path)\n",
    "    image = tensorflow.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "def load_dataset(\n",
    "    image_path, \n",
    "    classes, \n",
    "    bounderies\n",
    "):\n",
    "    image = load_image(image_path)\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tensorflow.cast(\n",
    "            classes, \n",
    "            dtype=tensorflow.float32\n",
    "        ),\n",
    "        \"boxes\": bounderies\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"images\": tensorflow.cast(\n",
    "            image, \n",
    "            dtype=tensorflow.float32\n",
    "        ), \n",
    "        \"bounding_boxes\": bounding_boxes\n",
    "    }\n",
    "\n",
    "resize = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.Resizing(\n",
    "            width=target_size_width, \n",
    "            height=target_size_height, \n",
    "            pad_to_aspect_ratio=True, \n",
    "            bounding_box_format=boundary_format\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "number_of_validation: int = int(size_of_annotations * split_at)\n",
    "\n",
    "validation_data = dataset.take(\n",
    "    number_of_validation\n",
    ")\n",
    "\n",
    "training_data = dataset.skip(\n",
    "    number_of_validation\n",
    ")\n",
    "\n",
    "training_data = training_data.map(\n",
    "    load_dataset, \n",
    "    num_parallel_calls=tensorflow.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "training_data = training_data.shuffle(batch_size * 4)\n",
    "\n",
    "training_data = training_data.ragged_batch(\n",
    "    batch_size, \n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "training_data = training_data.map(\n",
    "    resize, \n",
    "    num_parallel_calls=tensorflow.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "validation_data = validation_data.map(\n",
    "    load_dataset, \n",
    "    num_parallel_calls=tensorflow.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "validation_data = validation_data.shuffle(batch_size * 4)\n",
    "\n",
    "validation_data = validation_data.ragged_batch(\n",
    "    batch_size, \n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "validation_data = validation_data.map(\n",
    "    resize, \n",
    "    num_parallel_calls=tensorflow.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "def visualise_dataset(\n",
    "        dataset_as_input, \n",
    "        value_range, \n",
    "        rows, \n",
    "        cols, \n",
    "        bounding_box_format\n",
    "):\n",
    "    global labels_mapped\n",
    "    dataset_as_input = next(iter(dataset_as_input.take(1)))\n",
    "    images, bounding_boxes = dataset_as_input[\"images\"], dataset_as_input[\"bounding_boxes\"]\n",
    "    \n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.5,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=labels_mapped\n",
    "    )\n",
    "\n",
    "visualise_dataset_before_run: bool = False\n",
    "\n",
    "if visualise_dataset_before_run:\n",
    "    visualise_dataset(\n",
    "        training_data, \n",
    "        bounding_box_format=boundary_format, \n",
    "        value_range=(0, 255), \n",
    "        rows=2, \n",
    "        cols=2\n",
    "    )\n",
    "\n",
    "    visualise_dataset(\n",
    "        validation_data, \n",
    "        bounding_box_format=boundary_format, \n",
    "        value_range=(0, 255), \n",
    "        rows=2, \n",
    "        cols=2\n",
    "    )\n",
    "\n",
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "training_data = training_data.map(dict_to_tuple, num_parallel_calls=tensorflow.data.AUTOTUNE)\n",
    "training_data = training_data.prefetch(tensorflow.data.AUTOTUNE)\n",
    "\n",
    "validation_data = validation_data.map(dict_to_tuple, num_parallel_calls=tensorflow.data.AUTOTUNE)\n",
    "validation_data = validation_data.prefetch(tensorflow.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "from keras.layers import InputLayer\n",
    "from keras import Model\n",
    "\n",
    "# Model Creation\n",
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    \"yolo_v8_m_backbone_coco\" \n",
    ")\n",
    "\n",
    "yolo = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(labels_mapped),\n",
    "    bounding_box_format=boundary_format,\n",
    "    backbone=backbone,\n",
    "    fpn_depth=2\n",
    ")\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate,\n",
    "    global_clipnorm=global_clip_at,\n",
    ")\n",
    "\n",
    "yolo.compile(\n",
    "    optimizer=optimizer, \n",
    "    classification_loss=\"binary_crossentropy\", \n",
    "    box_loss=\"ciou\"\n",
    ")\n",
    "\n",
    "\n",
    "yolo.fit(\n",
    "    training_data,\n",
    "    validation_data=validation_data,\n",
    "    epochs=epoch,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    "    callbacks=[\n",
    "        WandbMetricsLogger()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name='yolo_model', \n",
    "    type='model'\n",
    ")\n",
    "\n",
    "artifact.add_dir(\n",
    "    local_path='D:\\\\Model\\\\yolo'\n",
    ")\n",
    "\n",
    "wandb.log_artifact(\n",
    "    artifact\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
